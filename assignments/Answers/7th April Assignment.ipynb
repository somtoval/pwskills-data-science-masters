{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d1b1415a",
   "metadata": {},
   "source": [
    "# Q1. What is the relationship between polynomial functions and kernel functions in machine learning algorithms?\n",
    "\n",
    "## Ans. :\n",
    "\n",
    "Polynomial functions and kernel functions are both used in machine learning algorithms to transform the input data into a higher-dimensional space in order to make it easier to classify or analyze.\n",
    "\n",
    "Polynomial functions are a type of function that can be used to transform data into a higher-dimensional space by raising the original input features to different powers and combining them. For example, if we have a two-dimensional input with features x and y, we can transform it into a higher-dimensional space by computing the values of x^2, y^2, and xy, and combining them to form a new set of features.\n",
    "\n",
    "Kernel functions, on the other hand, are functions that measure the similarity between two data points in a higher-dimensional space without actually computing the coordinates of the points in that space. This allows us to perform calculations in a high-dimensional space without actually having to compute the coordinates of the points in that space.\n",
    "\n",
    "One common type of kernel function used in machine learning is the polynomial kernel function, which is based on a polynomial transformation of the input data. In this case, the kernel function computes the inner product of the transformed data points in the high-dimensional space, which allows us to compute the similarity between them without actually computing the coordinates of the points in that space.\n",
    "\n",
    "In summary, polynomial functions can be used to transform data into a higher-dimensional space, and kernel functions can be used to measure the similarity between data points in that space. Polynomial kernel functions combine these two techniques by using a polynomial transformation to define the high-dimensional space and a kernel function to measure the similarity between data points in that space."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef5e4bde",
   "metadata": {},
   "source": [
    "# Q2. How can we implement an SVM with a polynomial kernel in Python using Scikit-learn?\n",
    "\n",
    "## Ans. :\n",
    "\n",
    "### To implement an SVM with a polynomial kernel in Python using Scikit-learn, we can follow these steps:\n",
    "\n",
    "### Import the necessary modules:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fe104ae0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import seaborn as sns\n",
    "from sklearn import datasets\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "from warnings import filterwarnings\n",
    "filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fedeedae",
   "metadata": {},
   "source": [
    "### Load the dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "dc63d87f",
   "metadata": {},
   "outputs": [],
   "source": [
    "iris = datasets.load_iris()\n",
    "X = iris.data[:, :2]  # we only take the first two features for visualization\n",
    "y = iris.target"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8245e60a",
   "metadata": {},
   "source": [
    "### Split the data into training and testing sets:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5edf6337",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4da14a8d",
   "metadata": {},
   "source": [
    "### Create an SVM model with a polynomial kernel:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2ed06608",
   "metadata": {},
   "outputs": [],
   "source": [
    "svm_poly = SVC(kernel='poly', degree=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0889ea8",
   "metadata": {},
   "source": [
    "### Train the SVM model on the training data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3944a607",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SVC(kernel='poly')"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "svm_poly.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be886be5",
   "metadata": {},
   "source": [
    "### Make predictions on the test data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "20b6f736",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = svm_poly.predict(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8ffe765",
   "metadata": {},
   "source": [
    "### Evaluate the accuracy of the model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c59db9c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.7555555555555555\n"
     ]
    }
   ],
   "source": [
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print('Accuracy:', accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b6c8b0c",
   "metadata": {},
   "source": [
    "# Q3. How does increasing the value of epsilon affect the number of support vectors in SVR?\n",
    "\n",
    "## Ans. :\n",
    "\n",
    "In Support Vector Regression (SVR), epsilon is a hyperparameter that determines the width of the margin of the epsilon-insensitive loss function. This margin defines a range of values for which errors are ignored and not penalized in the objective function.\n",
    "\n",
    "Increasing the value of epsilon in SVR typically results in an increase in the number of support vectors. This is because as the width of the margin increases, more data points fall within the margin and are classified as support vectors.\n",
    "\n",
    "Intuitively, increasing the value of epsilon makes the model more tolerant to errors, and thus allows more data points to be classified as support vectors. However, it's important to note that increasing epsilon can also lead to a decrease in the accuracy of the model, as a wider margin can lead to underfitting and decreased generalization performance.\n",
    "\n",
    "Therefore, the value of epsilon should be chosen carefully, taking into consideration the balance between model accuracy and the number of support vectors. In practice, the optimal value of epsilon is often found through cross-validation or grid search over a range of possible values."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3def3cef",
   "metadata": {},
   "source": [
    "# Q4. How does the choice of kernel function, C parameter, epsilon parameter, and gamma parameter affect the performance of Support Vector Regression (SVR)? Can you explain how each parameter works and provide examples of when you might want to increase or decrease its value?\n",
    "\n",
    "## Ans. :\n",
    "\n",
    "The choice of kernel function, C parameter, epsilon parameter, and gamma parameter can all have a significant impact on the performance of Support Vector Regression (SVR).\n",
    "\n",
    "1. __Kernel Function:__ The choice of kernel function determines the type of transformation that is applied to the data in order to create a nonlinear decision boundary. Different kernel functions are suited to different types of problems, and the optimal choice depends on the specific dataset and problem at hand. Examples of kernel functions include linear, polynomial, and radial basis function (RBF) kernels.\n",
    "\n",
    "2. __C Parameter:__ The C parameter determines the tradeoff between maximizing the margin and minimizing the error. A small value of C allows for a wider margin, but may result in more errors. A larger value of C results in a narrower margin, but fewer errors. In general, increasing the value of C increases the complexity of the model, which may improve performance on the training data, but may also increase the risk of overfitting.\n",
    "\n",
    "3. __Epsilon Parameter:__ The epsilon parameter determines the width of the margin of the epsilon-insensitive loss function, and affects the number of support vectors that are selected. A smaller value of epsilon leads to a smaller margin, which can result in fewer support vectors, but may also lead to overfitting. A larger value of epsilon leads to a wider margin, which can result in more support vectors and greater stability, but may also result in underfitting.\n",
    "\n",
    "4. __Gamma Parameter:__ The gamma parameter determines the shape of the decision boundary and the influence of each data point on the decision boundary. A small value of gamma results in a smoother decision boundary and may help to avoid overfitting. A larger value of gamma results in a more complex decision boundary and can help to capture more intricate patterns in the data. However, too large a value of gamma can lead to overfitting and poor generalization."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dee21615",
   "metadata": {},
   "source": [
    "# Q5. Assignment:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "72fca42b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the necessary libraries and load the dataset\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import seaborn as sns\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import accuracy_score\n",
    "import joblib\n",
    "from warnings import filterwarnings\n",
    "filterwarnings('ignore')\n",
    "\n",
    "iris = load_iris()\n",
    "X, y = iris.data, iris.target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3b8382f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the dataset into training and testing set:\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "33b3de43",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocess the data using any technique of your choice (e.g. scaling, normalization):\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "41c6ea2b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SVC(C=1, gamma=0.1)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create an instance of the SVC classifier and train it on the training data:\n",
    "clf = SVC(kernel='rbf', C=1, gamma=0.1)\n",
    "clf.fit(X_train_scaled, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "06fef2b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use the trained classifier to predict the labels of the testing data:\n",
    "y_pred = clf.predict(X_test_scaled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "088fe226",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 1.0\n"
     ]
    }
   ],
   "source": [
    "# Evaluate the performance of the classifier using any metric of your choice (e.g. accuracy, precision, recall, F1-score):\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(\"Accuracy:\", accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b6eacc2d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Parameters: {'C': 10, 'gamma': 0.1, 'kernel': 'linear'}\n"
     ]
    }
   ],
   "source": [
    "# Tune the hyperparameters of the SVC classifier using GridSearchCV or RandomizedSearchCV to improve its performance:\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "param_grid = {\n",
    "    'C': [0.1, 1, 10, 100],\n",
    "    'gamma': [0.1, 1, 10, 100],\n",
    "    'kernel': ['linear', 'rbf', 'poly']\n",
    "}\n",
    "\n",
    "grid_search = GridSearchCV(SVC(), param_grid, cv=5)\n",
    "grid_search.fit(X_train_scaled, y_train)\n",
    "\n",
    "print(\"Best Parameters:\", grid_search.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "747db422",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SVC(C=1, gamma=0.1)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Train the tuned classifier on the entire dataset:\n",
    "clf_tuned = SVC(kernel='rbf', C=1, gamma=0.1)\n",
    "clf_tuned.fit(X_train_scaled, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "48b3f639",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['svm_classifier.joblib']"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Save the trained classifier to a file for future use:\n",
    "joblib.dump(clf_tuned, 'svm_classifier.joblib')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
