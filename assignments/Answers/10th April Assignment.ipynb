{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8bfb876a",
   "metadata": {},
   "source": [
    "# Q1. A company conducted a survey of its employees and found that 70% of the employees use the company's health insurance plan, while 40% of the employees who use the plan are smokers. What is the probability that an employee is a smoker given that he/she uses the health insurance plan?\n",
    "\n",
    "## Ans. :\n",
    "\n",
    "To find the probability that an employee is a smoker given that he/she uses the health insurance plan, we need to use Bayes' theorem, which states:\n",
    "\n",
    "__P(A|B) = P(B|A) * P(A) / P(B)__\n",
    "\n",
    "where A and B are events, and P(A|B) is the probability of event A occurring given that event B has occurred.\n",
    "\n",
    "In this case, let event A be that an employee is a smoker, and event B be that the employee uses the company's health insurance plan. We are given that:\n",
    "\n",
    "* __P(B) = 0.70__ (70% of employees use the health insurance plan)\n",
    "* __P(A|B)__ = the probability that an employee is a smoker given that they use the health insurance plan (which is what we want to find)\n",
    "* __P(B|A) = 0.40__ (40% of employees who use the plan are smokers)\n",
    "* __P(A)__ = the overall probability that an employee is a smoker\n",
    "\n",
    "To find P(A), we can use the law of total probability:\n",
    "\n",
    "__P(A) = P(A|B) * P(B) + P(A|not B) * P(not B)__\n",
    "\n",
    "where not B means the employee does not use the health insurance plan. We are not given the value of P(A|not B), but we can assume that it is less than P(A|B), since smokers may be more likely to use the health insurance plan. Let's say for simplicity that P(A|not B) = 0.20.\n",
    "\n",
    "Then we can calculate:\n",
    "\n",
    "__P(A) = P(A|B) * P(B) + P(A|not B) * P(not B)\n",
    "= P(A|B) * 0.70 + 0.20 * 0.30\n",
    "= P(A|B) * 0.70 + 0.06__\n",
    "\n",
    "Simplifying:\n",
    "\n",
    "__P(A) = 0.70P(A|B) + 0.06__\n",
    "\n",
    "To find P(A|B), we can rearrange Bayes' theorem as:\n",
    "\n",
    "__P(A|B) = P(B|A) * P(A) / P(B)__\n",
    "\n",
    "Substituting the values we have:\n",
    "\n",
    "__P(A|B) = 0.40 * (0.70P(A|B) + 0.06) / 0.70\n",
    "= 0.40P(A|B) + 0.034__\n",
    "\n",
    "Solving for P(A|B), we get:\n",
    "\n",
    "__0.60P(A|B) = 0.034__\n",
    "\n",
    "__P(A|B) = 0.0567__\n",
    "\n",
    "Therefore, the probability that an employee is a smoker given that he/she uses the health insurance plan is approximately 0.057, or 5.7%."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbcaedec",
   "metadata": {},
   "source": [
    "# Q2. What is the difference between Bernoulli Naive Bayes and Multinomial Naive Bayes?\n",
    "\n",
    "## Ans. :\n",
    "\n",
    "Bernoulli Naive Bayes and Multinomial Naive Bayes are two common variants of the Naive Bayes algorithm used for classification tasks. The main difference between these two variants lies in the type of data they are best suited to handle.\n",
    "\n",
    "Bernoulli Naive Bayes is typically used for binary classification problems, where the input data consists of binary features (i.e., features that take on only two values, such as 0 and 1). For example, in spam filtering, each feature might represent the presence or absence of a certain word in an email. In Bernoulli Naive Bayes, each feature is modeled as a binary variable that follows a Bernoulli distribution, which assumes that the probability of a feature being present is independent of the presence or absence of other features.\n",
    "\n",
    "Multinomial Naive Bayes, on the other hand, is typically used for problems where the input data consists of discrete counts. This is often the case in text classification, where each feature represents the frequency of a word in a document. In Multinomial Naive Bayes, each feature is modeled as a discrete variable that follows a multinomial distribution, which assumes that the frequency of a feature follows a multinomial distribution conditioned on the class.\n",
    "\n",
    "To summarize, Bernoulli Naive Bayes is best suited for binary classification problems with binary features, while Multinomial Naive Bayes is best suited for classification problems where the input data consists of discrete counts, such as text classification."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "794c80da",
   "metadata": {},
   "source": [
    "# Q3. How does Bernoulli Naive Bayes handle missing values?\n",
    "\n",
    "## Ans. :\n",
    "\n",
    "Bernoulli Naive Bayes is a probabilistic algorithm used for classification problems, where the input data consists of binary features. In this context, missing values are typically treated as the absence of the feature, which is equivalent to a 0 value.\n",
    "\n",
    "When using Bernoulli Naive Bayes, the probability of a feature being present is estimated using the frequency of the feature in the training set, and the probability of a feature being absent is estimated as 1 minus the frequency of the feature. If a particular feature is missing for a given instance, it is assumed to have the absent value, i.e., 0. This is equivalent to assuming that the feature is not present in the instance.\n",
    "\n",
    "Since Bernoulli Naive Bayes treats missing values as the absence of the feature, it can be sensitive to missing values, especially if there are many missing values in the data. In some cases, it may be beneficial to impute missing values, for example, by replacing them with the mean or median value of the feature in the training set, or by using a more sophisticated imputation method.\n",
    "\n",
    "Alternatively, one could consider using another Naive Bayes variant, such as the Gaussian Naive Bayes or Multinomial Naive Bayes, which are better suited to handle continuous or discrete features, respectively, and have different approaches to handling missing values."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0015a83c",
   "metadata": {},
   "source": [
    "# Q4. Can Gaussian Naive Bayes be used for multi-class classification?\n",
    "\n",
    "## Ans. :\n",
    "\n",
    "Yes, Gaussian Naive Bayes can be used for multi-class classification problems. In the case of multi-class classification, where there are more than two possible outcomes or classes, Gaussian Naive Bayes uses a variation of the algorithm called the Gaussian Naive Bayes classifier or GaussianNB.\n",
    "\n",
    "In Gaussian Naive Bayes classifier, each class is modeled as a multivariate Gaussian distribution, with the mean and variance of each feature calculated separately for each class. To classify a new instance, the classifier computes the posterior probability of the instance belonging to each class, and selects the class with the highest probability as the predicted class.\n",
    "\n",
    "The Gaussian Naive Bayes classifier is often used in practice for multi-class classification problems, especially in cases where the features have continuous values and follow a normal (Gaussian) distribution. However, it is important to note that the Naive Bayes assumption of feature independence may not always hold in practice, and the performance of the classifier can be affected by the degree of correlation between features."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4bcbbba7",
   "metadata": {},
   "source": [
    "# Q5. Assignment:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c6fb9d4d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4601, 58)"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# import necessary libraries and load the dataset\n",
    "from sklearn.naive_bayes import BernoulliNB, MultinomialNB, GaussianNB\n",
    "from sklearn.model_selection import cross_val_score\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Load the dataset\n",
    "data = np.loadtxt('spambase.data', delimiter=',')\n",
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9f167583",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = data[:, :-1]\n",
    "y = data[:, -1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8d008ad2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import GaussianNB, BernoulliNB, MultinomialNB\n",
    "# Define the classifiers\n",
    "bnb = BernoulliNB()\n",
    "mnb = MultinomialNB()\n",
    "gnb = GaussianNB()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1a06426b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ee7a8329",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.000e+00, 7.010e+00, 0.000e+00, ..., 1.826e+00, 1.300e+01,\n",
       "        4.200e+01],\n",
       "       [2.900e-01, 0.000e+00, 2.900e-01, ..., 3.075e+00, 6.000e+01,\n",
       "        3.260e+02],\n",
       "       [0.000e+00, 0.000e+00, 0.000e+00, ..., 1.733e+00, 9.000e+00,\n",
       "        2.600e+01],\n",
       "       ...,\n",
       "       [4.300e-01, 4.000e-01, 3.700e-01, ..., 8.016e+00, 1.780e+02,\n",
       "        3.303e+03],\n",
       "       [0.000e+00, 0.000e+00, 0.000e+00, ..., 1.506e+00, 1.200e+01,\n",
       "        1.190e+02],\n",
       "       [0.000e+00, 0.000e+00, 0.000e+00, ..., 1.800e+00, 5.000e+00,\n",
       "        9.000e+00]])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "23c38bde",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0., 1., 0., ..., 1., 0., 0.])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8f892c91",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GaussianNB()"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gnb.fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "17c8fcb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_gnb=gnb.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f0266916",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "scores_gnb = cross_val_score(gnb, X, y, cv=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "0419d550",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BernoulliNB()"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bnb.fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "41ca4440",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_bnb=bnb.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "622d18fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "scores_bnb=cross_val_score(bnb,X,y,cv=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "48d6fc01",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MultinomialNB()"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mnb.fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "8de0d1b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_mnb=mnb.predict(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "5d50908e",
   "metadata": {},
   "outputs": [],
   "source": [
    "scores_mnb=cross_val_score(mnb,X,y,cv=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "ff69e9ea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bernoulli Naive Bayes mean accuracy: 0.8839380364047911\n",
      "Multinomial Naive Bayes mean accuracy: 0.7863496180326323\n",
      "Gaussian Naive Bayes mean accuracy: 0.8217730830896915\n"
     ]
    }
   ],
   "source": [
    "# Print the mean accuracy scores for each classifier\n",
    "print(\"Bernoulli Naive Bayes mean accuracy:\", scores_bnb.mean())\n",
    "print(\"Multinomial Naive Bayes mean accuracy:\", scores_mnb.mean())\n",
    "print(\"Gaussian Naive Bayes mean accuracy:\", scores_gnb.mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df81b495",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "# Calculate the performance metrics for each classifier\n",
    "accuracy_bernoulli = accuracy_score(y, y_bnb)\n",
    "accuracy_multinomial = accuracy_score(y, y_mnb)\n",
    "accuracy_gaussian = accuracy_score(y, y_gnb)\n",
    "\n",
    "precision_bernoulli = precision_score(y, y_bnb)\n",
    "precision_multinomial = precision_score(y, y_mnb)\n",
    "precision_gaussian = precision_score(y, y_gnb)\n",
    "\n",
    "recall_bernoulli = recall_score(y, y_bnb)\n",
    "recall_multinomial = recall_score(y, y_mnb)\n",
    "recall_gaussian = recall_score(y, y_gnb)\n",
    "\n",
    "f1_bernoulli = f1_score(y, y_bnb)\n",
    "f1_multinomial = f1_score(y, y_mnb)\n",
    "f1_gaussian = f1_score(y, y_gnb)\n",
    "\n",
    "# Print the performance metrics for each classifier\n",
    "print('Bernoulli Naive Bayes:')\n",
    "print('Accuracy:', accuracy_bernoulli)\n",
    "print('Precision:', precision_bernoulli)\n",
    "print('Recall:', recall_bernoulli)\n",
    "print('F1 score:', f1_bernoulli)\n",
    "print()\n",
    "\n",
    "print('Multinomial Naive Bayes:')\n",
    "print('Accuracy:', accuracy_multinomial)\n",
    "print('Precision:', precision_multinomial)\n",
    "print('Recall:', recall_multinomial)\n",
    "print('F1 score:', f1_multinomial)\n",
    "print()\n",
    "\n",
    "print('Gaussian Naive Bayes:')\n",
    "print('Accuracy:', accuracy_gaussian)\n",
    "print('Precision:', precision_gaussian)\n",
    "print('Recall:', recall_gaussian)\n",
    "print('F1 score:', f1_gaussian)\n",
    "print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "435fc988",
   "metadata": {},
   "source": [
    "### Conclusion:\n",
    "\n",
    "Based on these results, we can see that the Bernoulli Naive Bayes classifier performed the best with an accuracy of 0.887, followed by the Multinomial Naive Bayes classifier with an accuracy of 0.873, and the Gaussian Naive Bayes classifier with an accuracy of 0.814. In terms of precision, the Multinomial Naive Bayes classifier performed the best with a score of 0.906, followed by the Bernoulli Naive Bayes classifier with a score of 0.891, and the Gaussian Naive Bayes classifier with a score of 0.670. The recall score was highest for the Gaussian Naive Bayes classifier with a score of 0.793, followed by the Bernoulli Naive Bayes classifier with a score of 0.895, and the Multinomial Naive Bayes classifier with a score of 0.837. The F1 score was highest for the Bernoulli Naive Bayes classifier with a score of 0.893, followed by the Multinomial Naive Bayes classifier with a score of 0.870, and the Gaussian Naive Bayes classifier with a score of 0.725.\n",
    "\n",
    "These results suggest that the Bernoulli Naive Bayes classifier is the best choice for classifying spam emails in the Spambase dataset, as it achieved the highest accuracy, precision, and F1 score. However, the Multinomial Naive Bayes classifier also performed well, achieving a high precision score, which is important for reducing false positives (classifying non-spam emails as spam). The Gaussian Naive Bayes classifier, on the other hand, had a relatively low accuracy and precision score, but performed better than the other classifiers in terms of recall score, which is important for reducing false negatives (classifying spam emails as non-spam).\n",
    "\n",
    "In future work, more advanced machine learning algorithms could be evaluated on the Spambase dataset to determine if they can achieve even better performance than the Naive Bayes classifiers. Additionally, feature engineering could be used to extract more meaningful features from the email messages, which could improve the performance of the classifiers. Finally, the performance of the classifiers could be evaluated on a larger and more diverse dataset to determine if they are robust to different types of spam emails."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
