{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8447b695",
   "metadata": {},
   "source": [
    "# Q1. What is Gradient Boosting Regression?\n",
    "\n",
    "## Ans. :\n",
    "\n",
    "Gradient Boosting Regression is a popular machine learning technique used for supervised regression problems. It is an ensemble method that combines multiple weak predictive models (usually decision trees) to create a strong predictive model.\n",
    "\n",
    "In Gradient Boosting Regression, the algorithm builds an initial model and then iteratively adds more models to improve the prediction accuracy. Each subsequent model is trained on the residuals (the difference between the predicted and actual values) of the previous model. The residuals are used as the target variable for the next model, and the algorithm continues to add models until a predetermined stopping criterion is met.\n",
    "\n",
    "The \"gradient\" in the name refers to the use of gradient descent optimization to minimize the loss function between the predicted and actual values. The algorithm updates the parameters of each model to minimize the residual errors in the training data. The final prediction is the weighted sum of all the individual predictions from each model.\n",
    "\n",
    "Gradient Boosting Regression has several advantages, including its ability to handle non-linear relationships between features and the target variable, its flexibility in choosing loss functions, and its ability to handle missing data. However, it can be prone to overfitting if the model is too complex or the learning rate is too high."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1af7c9ae",
   "metadata": {},
   "source": [
    "# Q2. Implement a simple gradient boosting algorithm from scratch using Python and NumPy. Use a simple regression problem as an example and train the model on a small dataset. Evaluate the model's performance using metrics such as mean squared error and R-squared.\n",
    "\n",
    "## Ans. :\n",
    "\n",
    "__here's an implementation of a simple gradient boosting algorithm using Python and NumPy:__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88ba93f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "class GradientBoostingRegressor:\n",
    "    def __init__(self, n_estimators=100, learning_rate=0.1, max_depth=1):\n",
    "        self.n_estimators = n_estimators\n",
    "        self.learning_rate = learning_rate\n",
    "        self.max_depth = max_depth\n",
    "        self.models = []\n",
    "        self.intercept = 0\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        # Initialize the intercept as the mean of the target values\n",
    "        self.intercept = np.mean(y)\n",
    "        \n",
    "        # Initialize the residuals as the difference between the target values and the intercept\n",
    "        residuals = y - self.intercept\n",
    "        \n",
    "        # Build each model in the ensemble\n",
    "        for i in range(self.n_estimators):\n",
    "            # Train a decision tree to predict the residuals\n",
    "            tree = DecisionTreeRegressor(max_depth=self.max_depth)\n",
    "            tree.fit(X, residuals)\n",
    "            \n",
    "            # Make predictions with the current model and update the residuals\n",
    "            predictions = tree.predict(X)\n",
    "            residuals -= self.learning_rate * predictions\n",
    "            \n",
    "            # Add the model to the ensemble\n",
    "            self.models.append(tree)\n",
    "\n",
    "    def predict(self, X):\n",
    "        # Make predictions by summing the predictions from each model in the ensemble\n",
    "        predictions = np.zeros(X.shape[0]) + self.intercept\n",
    "        for model in self.models:\n",
    "            predictions += self.learning_rate * model.predict(X)\n",
    "        return predictions\n",
    "\n",
    "# Generate a small dataset for testing\n",
    "X = np.array([[1, 2], [3, 4], [5, 6], [7, 8]])\n",
    "y = np.array([3, 7, 11, 15])\n",
    "\n",
    "# Train the model and make predictions\n",
    "model = GradientBoostingRegressor(n_estimators=100, learning_rate=0.1, max_depth=1)\n",
    "model.fit(X, y)\n",
    "y_pred = model.predict(X)\n",
    "\n",
    "# Evaluate the model's performance using mean squared error and R-squared\n",
    "mse = np.mean((y - y_pred)**2)\n",
    "r2 = 1 - np.sum((y - y_pred)**2) / np.sum((y - np.mean(y))**2)\n",
    "\n",
    "print(\"Mean squared error:\", mse)\n",
    "print(\"R-squared:\", r2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2d98168",
   "metadata": {},
   "source": [
    "In this example, we generate a small dataset with 4 samples and 2 features, and the target variable is a linear function of the features. We then train a gradient boosting regressor with 100 decision trees, a learning rate of 0.1, and a maximum depth of 1. Finally, we evaluate the model's performance using mean squared error and R-squared. Note that in practice, we would use more sophisticated techniques to select the hyperparameters and evaluate the model's performance, such as cross-validation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d239ef9",
   "metadata": {},
   "source": [
    "# Q3. Experiment with different hyperparameters such as learning rate, number of trees, and tree depth to optimise the performance of the model. Use grid search or random search to find the best hyperparameters.\n",
    "\n",
    "## Ans. :\n",
    "\n",
    "__here's an example of how we could use grid search to optimize the hyperparameters of the gradient boosting regressor:__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea7cbfee",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_boston\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "\n",
    "# Load the Boston housing dataset\n",
    "boston = load_boston()\n",
    "X, y = boston.data, boston.target\n",
    "\n",
    "# Define the parameter grid for the grid search\n",
    "param_grid = {\n",
    "    'n_estimators': [100, 500, 1000],\n",
    "    'learning_rate': [0.01, 0.1, 1],\n",
    "    'max_depth': [1, 3, 5]\n",
    "}\n",
    "\n",
    "# Define the gradient boosting regressor\n",
    "gb = GradientBoostingRegressor()\n",
    "\n",
    "# Perform grid search with cross-validation\n",
    "grid_search = GridSearchCV(gb, param_grid, cv=5, scoring='neg_mean_squared_error', n_jobs=-1)\n",
    "grid_search.fit(X, y)\n",
    "\n",
    "# Print the best hyperparameters and the corresponding performance metrics\n",
    "print(\"Best parameters:\", grid_search.best_params_)\n",
    "y_pred = grid_search.predict(X)\n",
    "mse = mean_squared_error(y, y_pred)\n",
    "r2 = r2_score(y, y_pred)\n",
    "print(\"Mean squared error:\", mse)\n",
    "print(\"R-squared:\", r2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e415ba86",
   "metadata": {},
   "source": [
    "In this example, we use the Boston housing dataset, which is a regression problem with 13 features and a continuous target variable (median value of owner-occupied homes in thousands of dollars). We define a parameter grid with different values of the hyperparameters n_estimators, learning_rate, and max_depth, and use grid search with 5-fold cross-validation to find the best combination of hyperparameters that minimizes the negative mean squared error. Finally, we print the best hyperparameters and the corresponding performance metrics.\n",
    "\n",
    "Note that in practice, we would also use random search, which can be more efficient than grid search when the hyperparameter space is large. We would also use additional techniques such as early stopping to prevent overfitting and reduce computation time."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ff44b1a",
   "metadata": {},
   "source": [
    "# Q4. What is a weak learner in Gradient Boosting?\n",
    "\n",
    "## Ans. :\n",
    "\n",
    "In Gradient Boosting, a weak learner is a model that performs only slightly better than random guessing on a given problem. Weak learners are used as the base model in the ensemble, and their predictions are combined in a way that improves the overall performance of the model.\n",
    "\n",
    "In practice, decision trees are often used as weak learners in Gradient Boosting because they are simple and can be trained quickly. However, the trees are usually shallow (i.e., they have a small maximum depth), which limits their capacity to model complex relationships in the data. To compensate for this, Gradient Boosting trains a large number of trees and combines their predictions in a way that reduces the bias and variance of the model. The result is a model that is able to capture complex non-linear relationships between the features and the target variable.\n",
    "\n",
    "It's important to note that the term \"weak learner\" does not imply that the model is bad or that it has low accuracy. Rather, it refers to the fact that the model is only slightly better than random guessing, and that its predictions are improved by the boosting process."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "869ed7d9",
   "metadata": {},
   "source": [
    "# Q5. What is the intuition behind the Gradient Boosting algorithm?\n",
    "\n",
    "## Ans. :\n",
    "\n",
    "The intuition behind the Gradient Boosting algorithm is to sequentially add models to an ensemble in a way that corrects the errors of the previous models. The basic idea is to train a weak learner (such as a decision tree) on the original data, and then use the errors of the weak learner to adjust the target values for the next weak learner. This process is repeated iteratively, with each new model attempting to correct the errors of the previous models.\n",
    "\n",
    "The key to the Gradient Boosting algorithm is the use of gradients (i.e., the partial derivatives of the loss function with respect to the predictions) to adjust the target values for the next model. Specifically, for each iteration, the target values are adjusted by the negative gradient of the loss function with respect to the current predictions. This has the effect of \"pushing\" the predictions in the direction that minimizes the loss function.\n",
    "\n",
    "In each iteration, a new weak learner is trained on the adjusted target values, and its predictions are added to the predictions of the previous models. The process continues until a stopping criterion is met (such as a maximum number of iterations or a minimum improvement in the loss function).\n",
    "\n",
    "The intuition behind this process is that the weak learners are combined in a way that creates a powerful \"committee\" of models that can capture complex non-linear relationships between the features and the target variable. Each weak learner corrects the errors of the previous models, and the gradients are used to ensure that the corrections are made in the direction that minimizes the loss function. The result is a model that is able to achieve high accuracy on a wide range of prediction problems."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e2f45c7",
   "metadata": {},
   "source": [
    "# Q6. How does Gradient Boosting algorithm build an ensemble of weak learners?\n",
    "\n",
    "## Ans. :\n",
    "\n",
    "The Gradient Boosting algorithm builds an ensemble of weak learners in a sequential manner, by iteratively adding new weak learners to the ensemble and adjusting the target values used to train them based on the errors made by the previous models.\n",
    "\n",
    "The basic steps of the Gradient Boosting algorithm to build an ensemble of weak learners are as follows:\n",
    "\n",
    "1. Initialize the target values to be the true labels of the training examples.\n",
    "2. For each iteration, do the following:\n",
    "   * Train a weak learner on the training data, using the current target values as the labels.\n",
    "   * Calculate the predictions of the weak learner on the training data.\n",
    "   * Calculate the errors of the weak learner by subtracting its predictions from the current target values.\n",
    "   * Use the errors to adjust the target values for the next iteration, using a learning rate to control the amount of adjustment.\n",
    "   * Add the predictions of the weak learner to the ensemble of weak learners.\n",
    "3. Repeat the above steps until a stopping criterion is met (e.g., a maximum number of iterations is reached, or the improvement in the loss function is below a certain threshold).\n",
    "\n",
    "The key idea behind Gradient Boosting is that each new weak learner corrects the errors of the previous models, and the learning rate controls the contribution of each model to the ensemble. By using a large number of weak learners, and combining their predictions in a weighted manner, the Gradient Boosting algorithm is able to build a powerful model that can capture complex non-linear relationships between the features and the target variable.\n",
    "\n",
    "One important point to note is that the choice of weak learner can have a significant impact on the performance of the Gradient Boosting algorithm. In practice, decision trees are often used as weak learners because they are simple and can be trained quickly, but other models can also be used depending on the specific problem at hand."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "385ac806",
   "metadata": {},
   "source": [
    "# Q7. What are the steps involved in constructing the mathematical intuition of Gradient Boosting algorithm?\n",
    "\n",
    "## Ans. :\n",
    "\n",
    "The mathematical intuition behind the Gradient Boosting algorithm can be broken down into several steps:\n",
    "\n",
    "__1. Define a loss function:__ The first step in constructing the mathematical intuition of Gradient Boosting is to define a loss function that measures the error between the predictions of the model and the true values of the target variable. The loss function should be differentiable so that gradients can be computed.\n",
    "\n",
    "__2. Train a weak learner:__ The second step is to train a weak learner on the training data, using the current target values as the labels. The weak learner can be any model that performs only slightly better than random guessing on the given problem, such as a decision tree with a small maximum depth.\n",
    "\n",
    "__3. Calculate the errors:__ The third step is to calculate the errors of the weak learner by subtracting its predictions from the current target values. These errors represent the difference between the predictions of the weak learner and the true values of the target variable.\n",
    "\n",
    "__4. Adjust the target values:__ The fourth step is to adjust the target values for the next iteration based on the errors of the weak learner. This is done by adding the negative gradient of the loss function with respect to the predictions of the weak learner to the current target values. The learning rate is used to control the amount of adjustment.\n",
    "\n",
    "__5. Add the weak learner to the ensemble:__ The fifth step is to add the predictions of the weak learner to the ensemble of weak learners. This is done by combining the predictions of the weak learner with the predictions of the previous models using a weighted sum.\n",
    "\n",
    "__6. Repeat the process:__ The sixth step is to repeat the above steps until a stopping criterion is met (e.g., a maximum number of iterations is reached, or the improvement in the loss function is below a certain threshold).\n",
    "\n",
    "__7. Make final predictions:__ Once the ensemble of weak learners has been trained, the final predictions are made by combining the predictions of all the weak learners using a weighted sum.\n",
    "\n",
    "The key idea behind the Gradient Boosting algorithm is that each new weak learner corrects the errors of the previous models, and the learning rate controls the contribution of each model to the ensemble. By using a large number of weak learners, and combining their predictions in a weighted manner, the Gradient Boosting algorithm is able to build a powerful model that can capture complex non-linear relationships between the features and the target variable."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
