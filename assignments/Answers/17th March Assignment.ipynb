{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "18e23c1c",
   "metadata": {},
   "source": [
    "### Q1: What are missing values in a dataset? Why is it essential to handle missing values? Name some algorithms that are not affected by missing values."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff87adcb",
   "metadata": {},
   "source": [
    "Missing values in datasets refer to the absence or lack of recorded information for one or more variables or features in the data. When a dataset contains missing values, it means that the data point or observation is incomplete for certain attributes\n",
    "\n",
    "##### Why it is essential to handle missing values\n",
    "- Data Integrity: Missing values can introduce inconsistencies and inaccuracies in the dataset. Ignoring or mishandling missing values can lead to biased or incorrect results in data analysis and modeling.\n",
    "\n",
    "- Unbiased Analysis: Missing values can result in biased analysis and conclusions. For example, if the missing values are systematically related to certain characteristics or outcomes, excluding or ignoring them may lead to a distorted understanding of the data.\n",
    "\n",
    "- Model Performance: Missing values can impact the performance of predictive models. Models that are trained on incomplete data may not generalize well to new, unseen data and may provide unreliable predictions.\n",
    "\n",
    "- Data Interpretation: Missing values can affect the interpretation of relationships and patterns in the data. Incomplete data can lead to erroneous conclusions or misinterpretation of associations between variables.\n",
    "\n",
    "- Statistical Assumptions: Many statistical algorithms and techniques assume complete data or have assumptions about the distribution of the data. Violating these assumptions due to missing values can lead to incorrect statistical inferences and analysis.\n",
    "\n",
    "##### Algorithms that are not affected by missing vaues\n",
    "- Decision Trees: Decision trees can handle missing values by making decisions based on available features. They can create separate branches to handle missing values and continue splitting based on the available data.\n",
    "\n",
    "- Random Forests: Random forests are an ensemble of decision trees and inherit the ability to handle missing values. They can handle missing data by averaging predictions from multiple trees.\n",
    "\n",
    "- Gradient Boosting: Gradient boosting algorithms, such as XGBoost and LightGBM, can handle missing values by including missing value support in the algorithm. They can determine the best direction to handle missing values during the boosting process.\n",
    "\n",
    "- K-Nearest Neighbors (KNN): KNN algorithms impute missing values based on the values of the nearest neighbors. They use the available features to find the nearest data points and impute the missing values based on the values of those neighbors.\n",
    "\n",
    "- Naive Bayes: Naive Bayes algorithms can handle missing values by ignoring the missing data when estimating the probabilities during training and classification. They use available features to estimate the class probabilities."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74858492",
   "metadata": {},
   "source": [
    "### Q2: List down techniques used to handle missing data.  Give an example of each with python code."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f3becd8",
   "metadata": {},
   "source": [
    "##### Deleting colums or rows that contain null values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "be07c3a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7956cd82",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = sns.load_dataset('iris')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "66cbb830",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sepal_length</th>\n",
       "      <th>sepal_width</th>\n",
       "      <th>petal_length</th>\n",
       "      <th>petal_width</th>\n",
       "      <th>species</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5.1</td>\n",
       "      <td>3.5</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.2</td>\n",
       "      <td>setosa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4.9</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.2</td>\n",
       "      <td>setosa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4.7</td>\n",
       "      <td>3.2</td>\n",
       "      <td>1.3</td>\n",
       "      <td>0.2</td>\n",
       "      <td>setosa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4.6</td>\n",
       "      <td>3.1</td>\n",
       "      <td>1.5</td>\n",
       "      <td>0.2</td>\n",
       "      <td>setosa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5.0</td>\n",
       "      <td>3.6</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.2</td>\n",
       "      <td>setosa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>145</th>\n",
       "      <td>6.7</td>\n",
       "      <td>3.0</td>\n",
       "      <td>5.2</td>\n",
       "      <td>2.3</td>\n",
       "      <td>virginica</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>146</th>\n",
       "      <td>6.3</td>\n",
       "      <td>2.5</td>\n",
       "      <td>5.0</td>\n",
       "      <td>1.9</td>\n",
       "      <td>virginica</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>147</th>\n",
       "      <td>6.5</td>\n",
       "      <td>3.0</td>\n",
       "      <td>5.2</td>\n",
       "      <td>2.0</td>\n",
       "      <td>virginica</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>148</th>\n",
       "      <td>6.2</td>\n",
       "      <td>3.4</td>\n",
       "      <td>5.4</td>\n",
       "      <td>2.3</td>\n",
       "      <td>virginica</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>149</th>\n",
       "      <td>5.9</td>\n",
       "      <td>3.0</td>\n",
       "      <td>5.1</td>\n",
       "      <td>1.8</td>\n",
       "      <td>virginica</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>150 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     sepal_length  sepal_width  petal_length  petal_width    species\n",
       "0             5.1          3.5           1.4          0.2     setosa\n",
       "1             4.9          3.0           1.4          0.2     setosa\n",
       "2             4.7          3.2           1.3          0.2     setosa\n",
       "3             4.6          3.1           1.5          0.2     setosa\n",
       "4             5.0          3.6           1.4          0.2     setosa\n",
       "..            ...          ...           ...          ...        ...\n",
       "145           6.7          3.0           5.2          2.3  virginica\n",
       "146           6.3          2.5           5.0          1.9  virginica\n",
       "147           6.5          3.0           5.2          2.0  virginica\n",
       "148           6.2          3.4           5.4          2.3  virginica\n",
       "149           5.9          3.0           5.1          1.8  virginica\n",
       "\n",
       "[150 rows x 5 columns]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.dropna(axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2b6dd6e2",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sepal_length</th>\n",
       "      <th>sepal_width</th>\n",
       "      <th>petal_length</th>\n",
       "      <th>petal_width</th>\n",
       "      <th>species</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5.1</td>\n",
       "      <td>3.5</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.2</td>\n",
       "      <td>setosa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4.9</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.2</td>\n",
       "      <td>setosa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4.7</td>\n",
       "      <td>3.2</td>\n",
       "      <td>1.3</td>\n",
       "      <td>0.2</td>\n",
       "      <td>setosa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4.6</td>\n",
       "      <td>3.1</td>\n",
       "      <td>1.5</td>\n",
       "      <td>0.2</td>\n",
       "      <td>setosa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5.0</td>\n",
       "      <td>3.6</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.2</td>\n",
       "      <td>setosa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>145</th>\n",
       "      <td>6.7</td>\n",
       "      <td>3.0</td>\n",
       "      <td>5.2</td>\n",
       "      <td>2.3</td>\n",
       "      <td>virginica</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>146</th>\n",
       "      <td>6.3</td>\n",
       "      <td>2.5</td>\n",
       "      <td>5.0</td>\n",
       "      <td>1.9</td>\n",
       "      <td>virginica</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>147</th>\n",
       "      <td>6.5</td>\n",
       "      <td>3.0</td>\n",
       "      <td>5.2</td>\n",
       "      <td>2.0</td>\n",
       "      <td>virginica</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>148</th>\n",
       "      <td>6.2</td>\n",
       "      <td>3.4</td>\n",
       "      <td>5.4</td>\n",
       "      <td>2.3</td>\n",
       "      <td>virginica</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>149</th>\n",
       "      <td>5.9</td>\n",
       "      <td>3.0</td>\n",
       "      <td>5.1</td>\n",
       "      <td>1.8</td>\n",
       "      <td>virginica</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>150 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     sepal_length  sepal_width  petal_length  petal_width    species\n",
       "0             5.1          3.5           1.4          0.2     setosa\n",
       "1             4.9          3.0           1.4          0.2     setosa\n",
       "2             4.7          3.2           1.3          0.2     setosa\n",
       "3             4.6          3.1           1.5          0.2     setosa\n",
       "4             5.0          3.6           1.4          0.2     setosa\n",
       "..            ...          ...           ...          ...        ...\n",
       "145           6.7          3.0           5.2          2.3  virginica\n",
       "146           6.3          2.5           5.0          1.9  virginica\n",
       "147           6.5          3.0           5.2          2.0  virginica\n",
       "148           6.2          3.4           5.4          2.3  virginica\n",
       "149           5.9          3.0           5.1          1.8  virginica\n",
       "\n",
       "[150 rows x 5 columns]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.dropna(axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe46bc46",
   "metadata": {},
   "source": [
    "##### Median Value Imputation: Used when we have outliers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d1ca51b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['sepal_length'] = df['sepal_length'].fillna(df['sepal_length'].median())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad51d085",
   "metadata": {},
   "source": [
    "##### Mean Value Imputation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "9f5e4456",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['sepal_length'] = df['sepal_length'].fillna(df['sepal_length'].mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e56f676",
   "metadata": {},
   "source": [
    "##### Mode Value Imputation:  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "1db51dac",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['age_mode'] = df['sepal_length'].fillna(df['sepal_length'].mode())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec3ebe29",
   "metadata": {},
   "source": [
    "### Q3: Explain the imbalanced data. What will happen if imbalanced data is not handled?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25cca49b",
   "metadata": {},
   "source": [
    "Imbalanced data refers to a situation in machine learning or data analysis where the classes or categories being predicted are not represented equally. In other words, one class or category has significantly fewer instances or samples than the others.\n",
    "\n",
    "- Biased Predictions: Models trained on imbalanced data tend to be biased towards the majority class. They may favor predicting the majority class most of the time, resulting in poor detection or prediction performance for the minority class.\n",
    "\n",
    "- Poor generalization: Imbalanced data can negatively impact a model's ability to generalize to unseen data. Since the model is biased towards the majority class, it may struggle to correctly predict instances from the minority class in real-world scenarios where the class distribution may differ from the training set.\n",
    "\n",
    "- Misinterpretation of model performance: Traditional evaluation metrics like accuracy can be misleading when dealing with imbalanced data. A model that simply predicts the majority class for every instance can achieve a high accuracy, even though it fails to identify instances of the minority class. This can lead to a false sense of confidence in the model's performance."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa5c0ee3",
   "metadata": {},
   "source": [
    "### Q4: What are Up-sampling and Down-sampling? Explain with an example when up-sampling and downsampling are required."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4bc144a",
   "metadata": {},
   "source": [
    "Upsampling involves increasing the number of instances in the minority class to balance the class distribution. This can be done by replicating existing instances or by generating synthetic samples.\n",
    "\n",
    "Downsampling involves reducing the number of instances in the majority class to balance the class distribution. This can be done by randomly removing instances from the majority class or using more selective strategies.\n",
    "\n",
    "Upsampling (Oversampling) may be required when:\n",
    "\n",
    "- The minority class has a small number of instances compared to the majority class, and it is crucial to improve the representation of the minority class.\n",
    "- The available data for the minority class is limited, and generating synthetic samples can help increase diversity and capture the underlying patterns more effectively.\n",
    "- Overfitting is a concern, and replicating or generating synthetic samples can help address this issue by providing more training data.\n",
    "\n",
    "Downsampling (Undersampling) may be required when:\n",
    "\n",
    "- The majority class has a significantly larger number of instances, and it is important to reduce its dominance and rebalance the class distribution.\n",
    "- The computational resources or training time are limited, and reducing the dataset size by removing instances from the majority class can speed up the training process.\n",
    "- The majority class contains a considerable amount of noisy or redundant instances that may negatively impact the model's performance, and selective undersampling can help mitigate this issue."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e205c69",
   "metadata": {},
   "source": [
    "### Q5: What is data Augmentation? Explain SMOTE."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "206a5204",
   "metadata": {},
   "source": [
    "Data augmentation is a technique commonly used in machine learning and computer vision to artificially increase the size and diversity of a dataset by applying various transformations or modifications to the existing data. The goal of data augmentation is to introduce additional variations and patterns to improve the performance and generalization of machine learning models.\n",
    "\n",
    "SMOTE stands for Synthetic Minority Over-sampling Technique, and it is a popular data augmentation technique used to address the class imbalance problem in machine learning. SMOTE generates synthetic samples for the minority class by creating new instances that are a combination of existing minority class samples.\n",
    "\n",
    "The basic idea behind SMOTE is to interpolate between minority class samples to create synthetic instances in the feature space. Here's a step-by-step overview of how SMOTE works:\n",
    "\n",
    "- Select a minority class instance from the dataset.\n",
    "\n",
    "- Identify its k nearest neighbors (typically k is set to 5).\n",
    "\n",
    "- Randomly select one of the k nearest neighbors.\n",
    "\n",
    "- Generate a synthetic instance by combining the selected minority class instance and the randomly chosen neighbor. This is done by taking a weighted average of the feature values of the two instances. For example, if there are two features, the synthetic instance will have feature values that are midway between the original instance and the selected neighbor.\n",
    "\n",
    "- Repeat steps 1 to 4 for a desired number of synthetic instances to be generated.\n",
    "\n",
    "By repeating this process for multiple minority class instances, SMOTE augments the dataset with synthetic minority class samples. This helps to balance the class distribution and address the issue of class imbalance."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1968b612",
   "metadata": {},
   "source": [
    "### Q6: What are outliers in a dataset? Why is it essential to handle outliers?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d473014",
   "metadata": {},
   "source": [
    "Outliers are data points that significantly deviate from the overall pattern or distribution of the dataset. They are observations that are considerably different from the majority of other data points and may be indicative of errors, anomalies, or rare events.\n",
    "\n",
    "Reasons to handle Outliers\n",
    "\n",
    "- Accuracy of Analysis: Outliers can have a significant impact on the statistical measures and analysis results. They can distort the calculated values such as the mean, standard deviation, and correlation coefficients, leading to inaccurate insights and conclusions. By handling outliers appropriately, we can ensure the accuracy and reliability of the analysis.\n",
    "\n",
    "- Model Performance: Outliers can have a detrimental effect on the performance of machine learning and statistical models. These models often assume certain underlying distributions or patterns in the data, and outliers can violate those assumptions. Outliers can bias the models, lead to overfitting, or result in poor generalization to new data. By addressing outliers, we can improve the performance and robustness of the models.\n",
    "\n",
    "- Data Quality: Outliers may indicate data entry errors, measurement errors, or other data quality issues. Identifying and addressing outliers helps in improving the overall quality of the dataset by identifying potential errors or inconsistencies. It ensures that the data used for analysis or modeling is reliable and trustworthy."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8929e597",
   "metadata": {},
   "source": [
    "### Q7: You are working on a project that requires analyzing customer data. However, you notice that some of the data is missing. What are some techniques you can use to handle the missing data in your analysis?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc48fad0",
   "metadata": {},
   "source": [
    "- Deletion: If the records containing the missing values are very minimal the it can be deleted.\n",
    "\n",
    "- Imputation: Based on the feature, I can replace the missing value with mean, mode or median."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed2f9312",
   "metadata": {},
   "source": [
    "### Q8: You are working with a large dataset and find that a small percentage of the data is missing. What are some strategies you can use to determine if the missing data is missing at random or if there is a pattern to the missing data?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0a32e68",
   "metadata": {},
   "source": [
    "- Statistical Tests: Conduct statistical tests to assess the relationship between missingness and other variables. Perform tests such as chi-square tests or logistic regression to determine if missingness is associated with specific variables.\n",
    "\n",
    "- Expert Knowledge and Data Collection Process: Seek input from subject matter experts or individuals familiar with the data collection process. They might provide insights into the data collection methodology, potential biases, or reasons for missingness.\n",
    "\n",
    "- Sensitivity Analysis: Conduct sensitivity analysis by imputing missing values under different assumptions. Assess if the results vary significantly based on different assumptions about the missing data mechanism."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3add55ab",
   "metadata": {},
   "source": [
    "### Q9: Suppose you are working on a medical diagnosis project and find that the majority of patients in the dataset do not have the condition of interest, while a small percentage do. What are some strategies you can use to evaluate the performance of your machine learning model on this imbalanced dataset?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8dece7bd",
   "metadata": {},
   "source": [
    "- Use appropriate evaluation metrics: Accuracy alone may not be a suitable performance metric in imbalanced datasets due to the class imbalance. Instead, focus on evaluation metrics that are more informative, such as precision, recall, F1 score, or area under the receiver operating characteristic curve (ROC AUC). These metrics take into account the true positive, false positive, true negative, and false negative rates, providing a more comprehensive assessment of the model's performance.\n",
    "\n",
    "- Confusion matrix analysis: Analyze the confusion matrix to gain insights into the model's performance. This matrix displays the actual and predicted class labels, allowing you to examine true positives, true negatives, false positives, and false negatives. This analysis can help identify if the model is biased towards the majority class or if it has difficulty correctly classifying the minority class.\n",
    "\n",
    "- Class weighting or resampling techniques: Adjust the class weights or employ resampling techniques to address the class imbalance. Class weighting assigns higher weights to the minority class during model training, emphasizing its importance. Resampling techniques, such as oversampling the minority class (e.g., using SMOTE - Synthetic Minority Over-sampling Technique) or undersampling the majority class, can help rebalance the dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2f2451c",
   "metadata": {},
   "source": [
    "### Q10: When attempting to estimate customer satisfaction for a project, you discover that the dataset is unbalanced, with the bulk of customers reporting being satisfied. What methods can you employ to balance the dataset and down-sample the majority class?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce6b325d",
   "metadata": {},
   "source": [
    "- Random undersampling: Randomly select a subset of the majority class samples to match the number of samples in the minority class. This approach reduces the dataset size and rebalances the classes. However, it may discard potentially valuable information present in the majority class.\n",
    "\n",
    "- Stratified sampling: Stratify the dataset based on the target variable and perform random sampling from each stratum. This approach ensures that the proportion of satisfied and dissatisfied customers is maintained in the sampled dataset.\n",
    "\n",
    "- Tomek links: Identify pairs of samples from different classes that are the nearest neighbors of each other. Remove the majority class samples from these pairs, which are known as Tomek links. This technique emphasizes the decision boundary between the classes."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "faf99597",
   "metadata": {},
   "source": [
    "### Q11: You discover that the dataset is unbalanced with a low percentage of occurrences while working on a project that requires you to estimate the occurrence of a rare event. What methods can you employ to balance the dataset and up-sample the minority class?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5c7cded",
   "metadata": {},
   "source": [
    "- Random oversampling: Duplicate or replicate samples from the minority class randomly to increase their representation in the dataset. This approach increases the size of the minority class and balances the class distribution. However, it may lead to overfitting if not performed carefully.\n",
    "\n",
    "- SMOTE (Synthetic Minority Over-sampling Technique): Generate synthetic samples for the minority class by interpolating between neighboring samples. SMOTE creates synthetic samples by considering the feature space between each minority sample and its k nearest neighbors. This technique helps overcome the issue of overfitting associated with random oversampling.\n",
    "\n",
    "- Data augmentation techniques: Apply data augmentation techniques such as rotation, scaling, or flipping to the minority class samples. These techniques create variations of existing samples, increasing the diversity and size of the minority class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cf77ad4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50407f0e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77907cb8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3557aa7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55e41a49",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
