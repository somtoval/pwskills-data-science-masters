{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "904d6092",
   "metadata": {},
   "source": [
    "### Q1. Explain the difference between simple linear regression and multiple linear regression. Provide an example of each."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5c8dc43",
   "metadata": {},
   "source": [
    "Simple linear regression involves the use of a single independent variable to predict the value of a dependent variable. The relationship between the independent and dependent variables is assumed to be linear, meaning it can be represented by a straight line. Example: Let's say we want to predict a student's exam score (Y) based on the number of hours they studied (X).\n",
    "\n",
    "While Multiple linear regression expands on simple linear regression by incorporating two or more independent variables to predict the value of a dependent variable. It assumes a linear relationship between the dependent variable and multiple independent variables. Example: Let's say we want to predict a house's sale price (Y) based on its size (X₁), number of bedrooms (X₂), and age (X₃). "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0917f1e",
   "metadata": {},
   "source": [
    "### Q2. Discuss the assumptions of linear regression. How can you check whether these assumptions hold in a given dataset?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1dc2f7f9",
   "metadata": {},
   "source": [
    "Here are the key assumptions of linear regression:\n",
    "\n",
    "- Linearity: The relationship between the dependent variable and the independent variables is assumed to be linear. This means that the expected value of the dependent variable changes linearly with the independent variables.\n",
    "\n",
    "- Independence: The observations in the dataset are assumed to be independent of each other. This means that there is no correlation or dependence between the residuals or errors of the regression model.\n",
    "\n",
    "- Normality: The residuals are assumed to follow a normal distribution. This assumption allows for valid statistical inference, such as hypothesis testing and confidence interval estimation.\n",
    "\n",
    "- No multicollinearity: In multiple linear regression, the independent variables should not be highly correlated with each other. Multicollinearity can cause problems in interpreting the individual effects of the independent variables."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc176863",
   "metadata": {},
   "source": [
    "To check whether these assumptions hold in a given dataset, you can perform the following diagnostic tests:\n",
    "- Normality test: Plot a histogram or a Q-Q plot of the residuals and compare it to a normal distribution. \n",
    "- Residual analysis: Examine the residuals to check for patterns or systematic deviations from randomness. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ade2ed64",
   "metadata": {},
   "source": [
    "### Q3. How do you interpret the slope and intercept in a linear regression model? Provide an example using a real-world scenario."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41de52df",
   "metadata": {},
   "source": [
    "In linear regression slope and intercept represent the linear relationship between independent and dependent feature. The intercept represents the value of the dependent variable when all independent variables are zero, while the slope represents the change in the dependent variable for a one-unit change in the independent variable.\n",
    "\n",
    "For example, consider a linear regression model that predicts the salary of an individual based on their years of education. In this case, the intercept would represent the starting salary for someone with no years of education, which may not make sense in this context. A better interpretation would be to consider the slope coefficient, which represents the expected change in salary for every additional year of education. If the slope coefficient is 5, for example, it means that the expected increase in salary is $5,000 for every additional year of education."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a843109",
   "metadata": {},
   "source": [
    "### Q4. Explain the concept of gradient descent. How is it used in machine learning?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35ad1b9d",
   "metadata": {},
   "source": [
    "Gradient descent is a optimization algorithm which is used in machine learning to minimize the error function. The main idea behind gradient descent is to iteratively update the model's parameters by moving them in the direction of steepest descent of the cost function. The cost function measures the difference between the predicted values of the model and the actual values in the training data. The goal of gradient descent is to minimize this cost function and find the parameter values that result in the best fit to the data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3882f435",
   "metadata": {},
   "source": [
    "### Q5. Describe the multiple linear regression model. How does it differ from simple linear regression?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c08443b6",
   "metadata": {},
   "source": [
    "Multiple Linear Regression is one of the important regression algorithms which models the linear relationship between a single dependent continuous variable and more than one independent variable.\n",
    "\n",
    "example: multiple linear regression can be used to predict the sales of a product based on several independent variables, such as price, advertising expenditure, and customer demographics\n",
    "\n",
    "it is different than simple linear regression beacause in this the dataset has more than one or two independent features and one dependent features."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "814eda54",
   "metadata": {},
   "source": [
    "### Q6. Explain the concept of multicollinearity in multiple linear regression. How can you detect and address this issue?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fac173ab",
   "metadata": {},
   "source": [
    "Multicollinearity is common problem whic occurs when the dataset has more than one or two independent features and highly correlated with each others. This can lead to unreliable estimates of the regression coefficients, making it difficult to interpret the relationship between the independent variables and the dependent variable.\n",
    "\n",
    "To detect multicollinearity, one approach is to compute the correlation matrix of the independent variables. If the correlation coefficient between two independent variables is close to 1 or -1, this indicates that the variables are highly correlated."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2aa9c351",
   "metadata": {},
   "source": [
    "### Q7. Describe the polynomial regression model. How is it different from linear regression?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "317d0019",
   "metadata": {},
   "source": [
    "Polynomial mean non-linear relationship between two independent and dependent variable. One of the main differences between linear regression and polynomial regression is the shape of the relationship between the variables. Linear regression assumes that the relationship between x and y is a straight line, whereas polynomial regression can model more complex, curved relationships."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de200fa6",
   "metadata": {},
   "source": [
    "### Q8. What are the advantages and disadvantages of polynomial regression compared to linear regression? In what situations would you prefer to use polynomial regression?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a8624fd",
   "metadata": {},
   "source": [
    "Advantages:\n",
    "- Can model non-linear relationship\n",
    "- More flexible\n",
    "- Can improve accuracy\n",
    "\n",
    "Disadvantages:\n",
    "- Overfitting\n",
    "- Complexity\n",
    "- Extrapolation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db047b8a",
   "metadata": {},
   "source": [
    "In situations where the relationship between the variables is non-linear, polynomial regression may be preferred over linear regression. This includes cases where there are curvature or turning points in the relationship between the variables. However, care must be taken to avoid overfitting, and the degree of the polynomial should be chosen carefully based on the data and the complexity of the model. Additionally, if the relationship between the variables is truly linear, linear regression may be more appropriate and easier to interpret."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "975c9638",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
