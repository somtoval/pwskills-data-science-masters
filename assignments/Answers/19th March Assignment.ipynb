{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a91c1ad6",
   "metadata": {},
   "source": [
    "### Q1. What is Min-Max scaling, and how is it used in data preprocessing? Provide an example to illustrate its application"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6162303f",
   "metadata": {},
   "source": [
    "Min-max scaling, also known as normalization, is a popular technique used to rescale numerical features in a dataset to a specific range. The goal of min-max scaling is to transform the data so that it falls within a specified minimum and maximum range, typically between 0 and 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9a4ca24e",
   "metadata": {},
   "outputs": [],
   "source": [
    "my_val = [29,20,17,13,22,15,9]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "022874c1",
   "metadata": {},
   "source": [
    "Formular for the min-max scaler\n",
    "\n",
    "xscaled = \n",
    "    \n",
    "               xi - xmin   \n",
    "               \n",
    "              ____________\n",
    "              \n",
    "              xmax - xmin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a499fe7d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(29-9)/(29-9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cec95089",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.55"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(20-9)/(29-9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e3674116",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.4"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(17-9)/(29-9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2dbc2cc2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.2"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(13-9)/(29-9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "90b0f112",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.65"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(22-9)/(29-9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c463561b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.3"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(15-9)/(29-9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "90feafe7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(9-9)/(29-9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0318efd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "my_new_val = [1,0.55,0.4,0.2,0.65,0.3,0.0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c230be5c",
   "metadata": {},
   "source": [
    "### Q2. What is the Unit Vector technique in feature scaling, and how does it differ from Min-Max scaling? Provide an example to illustrate its application."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7459b32",
   "metadata": {},
   "source": [
    "Unit vector scaling, also known as vector normalization, is a technique used for feature scaling in which each data point (vector) is transformed to have a length of 1, while preserving the direction of the vector. It is commonly used in machine learning and data analysis to ensure that all features have equal importance and to prevent the dominance of features with larger magnitudes.\n",
    "\n",
    "Unit vector scaling normalizes each feature vector by dividing it by its Euclidean norm (magnitude), resulting in unit-length vectors. It focuses on preserving the direction of the vectors while equalizing their scales while Min-max scaling rescales each feature linearly to a specific range, typically between 0 and 1. It subtracts the minimum value and divides by the range (maximum value minus minimum value) to bring the values within the desired range."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ee36dca",
   "metadata": {},
   "source": [
    "Applying the unit vector scaling technique to normalize the feature vectors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "386395ee",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Student</th>\n",
       "      <th>Math Score</th>\n",
       "      <th>English Score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>80</td>\n",
       "      <td>75</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>90</td>\n",
       "      <td>85</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>70</td>\n",
       "      <td>65</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>85</td>\n",
       "      <td>95</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Student  Math Score  English Score\n",
       "0        1          80             75\n",
       "1        2          90             85\n",
       "2        3          70             65\n",
       "3        4          85             95"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.DataFrame({\n",
    "    'Student': [1,2,3,4],\n",
    "    'Math Score' : [80,90,70,85],\n",
    "    'English Score': [75,85,65,95]\n",
    "})\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b67b8cc6",
   "metadata": {},
   "source": [
    "To apply the unit vector scaling technique, we'll follow these steps:\n",
    "\n",
    "1. Calculate the Euclidean norm (magnitude) for each feature vector:\n",
    "\n",
    "> For Student 1: Magnitude = sqrt(Math Score^2 + English Score^2) = sqrt(80^2 + 75^2) = sqrt(13750) ≈ 117.32\n",
    "For Student 2: Magnitude = sqrt(90^2 + 85^2) = sqrt(15745) ≈ 125.39\n",
    "For Student 3: Magnitude = sqrt(70^2 + 65^2) = sqrt(9405) ≈ 97.00\n",
    "For Student 4: Magnitude = sqrt(85^2 + 95^2) = sqrt(15550) ≈ 124.62\n",
    "\n",
    "2. Divide each feature vector by its magnitude to obtain the unit vector:\n",
    "\n",
    "- For Student 1:\n",
    "Unit Vector = (Math Score / Magnitude, English Score / Magnitude) = (80 / 117.32, 75 / 117.32) ≈ (0.682, 0.640)\n",
    "- For Student 2:\n",
    "Unit Vector = (90 / 125.39, 85 / 125.39) ≈ (0.717, 0.679)\n",
    "- For Student 3:\n",
    "Unit Vector = (70 / 97.00, 65 / 97.00) ≈ (0.722, 0.670)\n",
    "- For Student 4:\n",
    "Unit Vector = (85 / 124.62, 95 / 124.62) ≈ (0.682, 0.762)\n",
    "\n",
    "Now, the feature vectors for Math and English scores are normalized to unit-length vectors. The direction of each vector is preserved, while the magnitude is equalized, making it easier to compare and analyze the relationship between the two subjects."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6e04366",
   "metadata": {},
   "source": [
    "### Q3. What is PCA (Principle Component Analysis), and how is it used in dimensionality reduction? Provide an example to illustrate its application."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51800c76",
   "metadata": {},
   "source": [
    "PCA (Principal Component Analysis) is a dimensionality reduction technique used to transform a high-dimensional dataset into a lower-dimensional representation. It aims to capture the most important patterns, correlations, and variations in the data by identifying the principal components.\n",
    "\n",
    "It is a commonly used as a dimensionality reduction technique to reduce the number of features (dimensions) in a dataset while retaining most of the information. By transforming the original data into a lower-dimensional space, PCA helps in simplifying the analysis, improving computational efficiency, and addressing the curse of dimensionality."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "914df90d",
   "metadata": {},
   "source": [
    "Let's consider an example of using PCA for dimensionality reduction in a dataset of customer purchasing behavior. Suppose we have a dataset that contains information about customers and the products they have purchased. The dataset includes various features such as age, income, gender, and the number of purchases for different product categories.\n",
    "\n",
    "However, the dataset has a high dimensionality due to the large number of features, which can make it challenging to analyze and visualize the data effectively. In this case, we can apply PCA to reduce the dimensionality of the dataset while preserving the key patterns and variations in the customer purchasing behavior."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53f6f5a4",
   "metadata": {},
   "source": [
    "### Q4. What is the relationship between PCA and Feature Extraction, and how can PCA be used for Feature Extraction? Provide an example to illustrate this concept."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ae2a095",
   "metadata": {},
   "source": [
    "PCA and feature extraction are closely related concepts, and PCA can be seen as a specific method for feature extraction. Feature extraction refers to the process of transforming the original features of a dataset into a new set of features that capture the most important information or patterns in the data.\n",
    "\n",
    "PCA can be used for feature extraction by transforming the original features of a dataset into a new set of features called principal components. These principal components capture the most important patterns and variations in the data, allowing for dimensionality reduction and improved interpretability. Here's a step-by-step process of using PCA for feature extraction:\n",
    "\n",
    "- Standardize the data: If the features in the dataset have different scales, it is important to standardize them (subtract the mean and divide by the standard deviation) to ensure they have equal weight in the analysis.\n",
    "\n",
    "- Compute the covariance matrix: Calculate the covariance matrix of the standardized data. The covariance matrix provides information about the relationships and variances between pairs of features.\n",
    "\n",
    "- Compute the eigenvectors and eigenvalues: Perform an eigendecomposition of the covariance matrix to obtain the eigenvectors and eigenvalues. The eigenvectors represent the principal components, and the eigenvalues indicate the amount of variance captured by each principal component. Sort the eigenvectors in decreasing order of their corresponding eigenvalues.\n",
    "\n",
    "- Select the principal components: Determine the number of principal components (k) to retain based on the desired level of dimensionality reduction. This can be determined by looking at the cumulative explained variance ratio, which represents the proportion of the total variance explained by the selected principal components. Select the top k eigenvectors.\n",
    "\n",
    "- Project the data: Transform the standardized data by projecting it onto the selected principal components. This involves taking the dot product between the standardized data and the eigenvectors corresponding to the selected principal components. The resulting lower-dimensional representation will have k features (principal components)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18414d0e",
   "metadata": {},
   "source": [
    "### Q5. You are working on a project to build a recommendation system for a food delivery service. The dataset contains features such as price, rating, and delivery time. Explain how you would use Min-Max scaling to preprocess the data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "548f9782",
   "metadata": {},
   "source": [
    "Here's how you can use Min-Max scaling to preprocess the data:\n",
    "\n",
    "- Identify the features: In this case, the features are price, rating, and delivery time.\n",
    "\n",
    "- Compute the minimum and maximum values: Calculate the minimum and maximum values for each feature in the dataset. For example, find the minimum and maximum price, rating, and delivery time values across all the data points.\n",
    "\n",
    "- Scale the values: For each feature, apply the Min-Max scaling formula to rescale the values between 0 and 1:\n",
    "\n",
    "> scaled_value = (original_value - min_value) / (max_value - min_value)\n",
    "\n",
    "> Substitute the original value with the respective feature value, and min_value and max_value with the calculated minimum and maximum values for that feature.\n",
    "\n",
    "- Repeat the process for all data points: Iterate through the dataset and apply the Min-Max scaling formula to each data point for each feature individually. This ensures that each data point is scaled based on the minimum and maximum values of its respective feature.\n",
    "\n",
    "After performing Min-Max scaling, the values of the features will be transformed to the range between 0 and 1. This normalization allows for fair comparison and combination of the features during the recommendation system's analysis."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc62062c",
   "metadata": {},
   "source": [
    "### Q6. You are working on a project to build a model to predict stock prices. The dataset contains many features, such as company financial data and market trends. Explain how you would use PCA to reduce the dimensionality of the dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0daf021",
   "metadata": {},
   "source": [
    "When working on a project to predict stock prices with a dataset containing numerous features, PCA can be used to reduce the dimensionality of the dataset while retaining the most important information and patterns. Here's how you can use PCA for dimensionality reduction in this context:\n",
    "\n",
    "- Dataset preparation: Collect a dataset that includes various features related to company financial data and market trends for multiple stocks. The dataset should have a sufficient number of data points.\n",
    "\n",
    "- Data preprocessing: Standardize the dataset by subtracting the mean and dividing by the standard deviation for each feature. This step ensures that all features have equal weight in the PCA analysis.\n",
    "\n",
    "- Compute the covariance matrix: Calculate the covariance matrix of the standardized dataset. The covariance matrix provides information about the relationships and variances between pairs of features.\n",
    "\n",
    "- Compute the eigenvectors and eigenvalues: Perform an eigendecomposition of the covariance matrix to obtain the eigenvectors and eigenvalues. The eigenvectors represent the principal components, and the eigenvalues indicate the amount of variance captured by each principal component. Sort the eigenvectors in decreasing order of their corresponding eigenvalues.\n",
    "\n",
    "- Select the principal components: Determine the number of principal components (k) to retain based on the desired level of dimensionality reduction. This can be determined by looking at the cumulative explained variance ratio or by setting a threshold for the amount of variance to retain. The cumulative explained variance ratio represents the proportion of the total variance explained by the selected principal components. Select the top k eigenvectors.\n",
    "\n",
    "- Project the data: Transform the standardized dataset by projecting it onto the selected principal components. This involves taking the dot product between the standardized data and the eigenvectors corresponding to the selected principal components. The resulting lower-dimensional representation will have k features (principal components).\n",
    "\n",
    "By applying PCA, the dataset is transformed from a high-dimensional space to a lower-dimensional space while retaining the most significant patterns and variations in the data. The reduced set of principal components captures the major sources of variability in the dataset.\n",
    "\n",
    "The advantage of using PCA for dimensionality reduction in the context of stock price prediction is that it can help mitigate the curse of dimensionality, reduce noise, and remove redundant or less informative features. By focusing on the principal components that explain the most variance, the dimensionality of the dataset is reduced, simplifying subsequent analysis and potentially improving the performance of the predictive model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2449ea7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e922a7f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "ccaccbe2",
   "metadata": {},
   "source": [
    "### Q7. For a dataset containing the following values: [1, 5, 10, 15, 20], perform Min-Max scaling to transform the values to a range of -1 to 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "7f13703b",
   "metadata": {},
   "outputs": [],
   "source": [
    "vals = [1, 5, 10, 15, 20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "90035178",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(1-1)/(20-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "2d3a810e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.21052631578947367"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(5-1)/(20-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "aa664198",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.47368421052631576"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(10-1)/(20-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "7cae3f22",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7368421052631579"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(15-1)/(20-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "0baf5293",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(20-1)/(20-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "5cca0f71",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_vals = [0.0,0.21,0.47,0.73,1.0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6676db45",
   "metadata": {},
   "source": [
    "### Q8. For a dataset containing the following features: [height, weight, age, gender, blood pressure], perform Feature Extraction using PCA. How many principal components would you choose to retain, and why?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4297728",
   "metadata": {},
   "source": [
    "Performing Feature Extraction using PCA involves identifying the principle components that capture the most variance in the data and using them as the new set of features. In this case, the dataset contains five features: height, weight, age, gender, and blood pressure.\n",
    "\n",
    "Before applying PCA, we need to preprocess the data by standardizing it to have zero mean and unit variance. We can then compute the covariance matrix of the data and find the principle components using eigendecomposition.\n",
    "\n",
    "The number of principal components to retain depends on the percentage of variance we want to preserve in the data. A common rule of thumb is to choose the smallest number of principal components that capture at least 70-80% of the variance in the data.\n",
    "\n",
    "To determine the number of principal components to retain for this dataset, we can compute the explained variance ratio for each principle component, which represents the proportion of the total variance in the data that is explained by each component.\n",
    "\n",
    "Once we have computed the explained variance ratio for each component, we can plot a scree plot to visualize the proportion of variance explained by each principal component. The scree plot shows a diminishing returns relationship between the number of principal components and the amount of variance explained. We can then choose the number of principal components that capture a high proportion of the variance while avoiding overfitting the data.\n",
    "\n",
    "Without any knowledge of the dataset or its characteristics, it is difficult to determine the number of principal components that should be retained. However, as a general guideline, retaining 2-3 principal components may be a good starting point as they would capture the most significant variability in the data.\n",
    "\n",
    "Ultimately, the optimal number of principal components to retain depends on the specifics of the dataset and the analysis being performed. It may require some experimentation and evaluation to determine the optimal number of principal components to retain."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16247fbf",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d38639e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "849fd3e5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e85b96bc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb21882a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8eb9f95a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3919bccc",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
